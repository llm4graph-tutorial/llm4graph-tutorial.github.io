<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>The Web Conference (WWW) 2024 Tutorial: Large Language Models for Graphs: Progresses and Directions</title>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">
              <span style="font-size: 80%">The Web Conference (WWW) 2024 Tutorial:</span><br />
              <span style="font-size: 85%">Large Language Models for Graphs: Progresses and Directions</span>
            </h1>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <table>
                <tr>
                  <!-- <th scope="row">TR-7</th> -->
                  <td width="16%" style="text-align: center; padding: 10px"><img width="150px" height="150px" src="static/imgs/profile_chaohuang.png"></td>
                  <td width="16%" style="text-align: center; padding: 10px"><img width="150px" height="150px" src="static/imgs/profile_xubinren.png"></td>
                  <td width="16%" style="text-align: center; padding: 10px"><img width="150px" height="150px" src="static/imgs/profile_jiabintang.png"></td>
                  <td width="16%" style="text-align: center; padding: 10px"><img width="150px" height="150px" src="static/imgs/profile_lianghaoxia.png"></td>
                  <td width="16%" style="text-align: center; padding: 10px"><img width="150px" height="150px" src="static/imgs/profile_daweiyin.png"></td>
                  <td width="16%" style="text-align: center; padding: 10px"><img width="150px" height="150px" src="static/imgs/profile_niteshchawla.png"></td>
              </tr>
                <tr>
                  <!-- <th scope="row">TR-7</th> -->
                  <td width="16%" style="text-align: center"><a href="https://sites.google.com/view/chaoh/home" style="border-radius: 50%">Chao Huang</a><sup>1</sup>,</td>
                  <td width="16%" style="text-align: center"><a href="https://ren-xubin.github.io/" style="border-radius: 50%">Xubin Ren</a><sup>1</sup>,</td>
                  <td width="16%" style="text-align: center"><a href="https://tjb-tech.github.io/" style="border-radius: 50%">Jiabin Tang</a><sup>1</sup>,</td>
                  <td width="16%" style="text-align: center"><a href="https://akaxlh.github.io/" style="border-radius: 50%">Lianghao Xia</a><sup>1</sup>,</td>
                  <td width="16%" style="text-align: center"><a href="https://www.yindawei.com/" style="border-radius: 50%">Dawei Yin</a><sup>2</sup>,</td>
                  <td width="16%" style="text-align: center"><a href="https://niteshchawla.nd.edu/" style="border-radius: 50%">Nitesh Chawla</a><sup>3</sup></td>
                </tr>
            </table>
            </span>
          </div>
          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>1</sup>The University of Hong Kong </span>
            <span class="author-block"><sup>2</sup>Baidu Inc</span>
            <span class="author-block"><sup>3</sup>University of Notre Dame</span>
          </div>
          <br />
          <div class="is-size-5 publication-authors">
            <b>Monday May 13 13:30 PM - 17:00 PM (SST) @ Resorts World Convention Centre/ Virgo 4 </b>
          </div>
          <!-- <div class="is-size-5 publication-authors">
            Zoom link available on <a href="https://underline.io/events/395/sessions?eventSessionId=15330&searchGroup=lecture" target="_blank">Underline</a>
          </div> -->
        </div>
      </div>
    </div>
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">About this tutorial</h2>
        <div class="content has-text-justified">
          <h2>
            <a href="https://github.com/HKUDS/Awesome-LLM4Graph-Papers">https://github.com/HKUDS/Awesome-LLM4Graph-Papers</a>
          </h2>
          <p>
            Graphs are an essential data structure utilized to represent relationships in real-world scenarios. Prior research has established that Graph Neural Networks (GNNs) deliver impressive outcomes in graph-centric tasks, such as link prediction and node classification. Despite these advancements, challenges like data sparsity and limited generalization capabilities continue to persist. Recently, Large Language Models (LLMs) have gained attention in natural language processing. They excel in language comprehension and summarization. Integrating LLMs with graph learning techniques has attracted interest as a way to enhance performance in graph learning tasks.
          </p>
          <p>
            We will explore four primary categories of methods that harness the power of large language models (LLMs) in graph tasks. These are: i) Graph Neural Networks (GNNs) as Prefix, ii) LLMs as Prefix, iii) LLMs-Graphs Integration, and iv) LLMs-Only.
            In each section, we will introduce you to the leading techniques in this field and provide you with sample code snippets to experiment with.
            This tutorial is designed to be valuable for both researchers aiming to pioneer new LLM4Graph solutions and industry professionals seeking to apply these methods in practical, real-world scenarios.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">

  

  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Slides</h2>
        Section 1: <a href="" target='_blank'>Introduction</a>
        <br />
        Section 2: <a href="" target='_blank'>GNNs as Prefix</a>
        <br />
        Section 3: <a href="" target='_blank'>LLMs as Prefix</a>
        <br />
        Section 4: <a href="" target='_blank'>LLMs-Graphs Intergration</a>
        <br />
        Section 5: <a href="" target='_blank'>LLMs-Only</a>

      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Schedule</h2>
        

        <div class="content has-text-justified">

          <style type="text/css">
          .tg  {border-collapse:collapse;border-spacing:0;}
          .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
            overflow:hidden;padding:10px 5px;word-break:normal;}
          .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
            font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
          .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
          .tg .tg-0lax{text-align:left;vertical-align:top}
          </style>
          <table class="tg">
          <thead>
            <tr>
              <th class="tg-0pky">Time</th>
              <th class="tg-0lax">Section</th>
              <th class="tg-0lax">Presenter</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td class="tg-0lax">13:30 - 13:40</td>
              <td class="tg-0lax">Opening & Introduction</td>
              <td class="tg-0lax">Chao Huang</td>
            </tr>
            <tr>
              <td class="tg-0lax">13:40 - 14:20</td>
              <td class="tg-0lax">Section 1: GNNs as Prefix</td>
              <td class="tg-0lax">Jiabin Tang</td>
            </tr>
            <tr>
              <td class="tg-0lax">14:20 - 14:25</td>
              <td class="tg-0lax">Q & A for Section 1</td>
              <td class="tg-0lax">Jiabin Tang</td>
            </tr>
            <tr>
              <td class="tg-0lax">14:25 - 15:05</td>
              <td class="tg-0lax">Section 2: LLMs as Prefix</td>
              <td class="tg-0lax">Lianghao Xia</td>
            </tr>
            <tr>
              <td class="tg-0lax">15:05 - 15:10</td>
              <td class="tg-0lax">Q & A for Section 2</td>
              <td class="tg-0lax">Lianghao Xia</td>
            </tr>
            <tr>
              <td class="tg-0lax">15:10 - 15:40</td>
              <td class="tg-0lax">Coffee Break</td>
              <td class="tg-0lax">-</td>
            </tr>
            <tr>
              <td class="tg-0lax">15:40 - 16:10</td>
              <td class="tg-0lax">Section 3: LLMs-Graphs Intergration</td>
              <td class="tg-0lax">Xubin Ren</td>
            </tr>
            <tr>
              <td class="tg-0lax">16:10 - 16:15</td>
              <td class="tg-0lax">Q & A for Section 3</td>
              <td class="tg-0lax">Xubin Ren</td>
            </tr>
            <tr>
              <td class="tg-0lax">16:15 - 16:55</td>
              <td class="tg-0lax">Section 4: LLMs-Only</td>
              <td class="tg-0lax">Xubin Ren & Jiabin Tang</td>
            </tr>
            <tr>
              <td class="tg-0lax">16:55 - 17:00</td>
              <td class="tg-0lax">Q & A for Section 4</td>
              <td class="tg-0lax">Xubin Ren & Jiabin Tang</td>
            </tr>
          </tbody>
          </table>
        </div>
      </div>
    </div>

   
    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Reading List</h2>

        <p>Works in our group are highlighted in <b>bold</b>.</p>

        <br />        
        <h3 class="title is-4">Section 1: GNNs as Prefix</h3>
        <h4 class="title is-5">3.1 Node- level Tokenization</h4>
        <ul>
          <li><a href="https://arxiv.org/pdf/2310.13023"><b>GraphGPT: Graph instruction tuning for large language models</b></a></li>
          <li><a href="https://arxiv.org/pdf/2402.16024v1"><b>HiGPT: Heterogeneous Graph Language Model</b></a></li>
          <li><a href="https://arxiv.org/pdf/2402.07197">GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks</a></li>
          <li><a href="https://arxiv.org/pdf/2402.13630v1">UniGraph: Learning a Cross-Domain Graph Foundation Model From Natural Language</a></li>
          <li><a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/129033c7c08be683059559e8d6bfd460-Paper-Conference.pdf">GIMLET:Aunifiedgraph-textmodelforinstruction-based molecule zero-shot learning</a></li>
        </ul>

        <br />
        
        <h4 class="title is-5">3.1 Node- level Tokenization</h4>
        <ul>
          <li><a href="https://arxiv.org/pdf/2310.05845">GraphLLM: Boosting graph reasoning ability of large language model</a></li>
          <li><a href="https://arxiv.org/pdf/2308.06911">GIT-Mol: A multi-modal large language model for molecular science with graph, image, and text</a></li>
          <li><a href="https://arxiv.org/pdf/2310.12798">MolCA: Molecular graph-language modeling with cross-modal projector and uni-modal adapter</a></li>
          <li><a href="https://arxiv.org/pdf/2311.16208">InstructMol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery</a></li>
          <li><a href="https://arxiv.org/pdf/2402.07630">G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering</a></li>
          <li><a href="https://ojs.aaai.org/index.php/AAAI/article/view/29875">Graph neural prompting with large language models</a></li>
        </ul>
        
        <br />
        
        <h3 class="title is-4">Section 2: LLMs as Prefix</h3>
        <h4 class="title is-5">2.1 Embeddings from LLMs for GNNs</h4>

        <ul>
          <li><a href="https://arxiv.org/pdf/2309.02848">Prompt-based node feature extractor for few-shot learning on text-attributed graphs</a></li>
          <li><a href="https://arxiv.org/pdf/2308.02565">SimTeG: A frustratingly simple approach improves textual graph learning</a></li>
          <li><a href="https://dl.acm.org/doi/pdf/10.1145/3580305.3599833">Graph-aware language model pre-training on a large graph corpus can help multiple graph applications</a></li>
          <li><a href="https://arxiv.org/pdf/2310.00149">One for all: Towards training one graph model for all classification tasks</a></li>
          <li><a href="https://openreview.net/pdf?id=RXFVcynVe1">Harnessing explanations: Llm-to-lm interpreter for enhanced text-attributed graph representation learning</a></li>
          <li><a href="https://arxiv.org/pdf/2311.00423v6"><b>LLMRec: Large language models with graph augmentation for recommendation</b></a></li>
        </ul>
        
        <br />
        
        <h4 class="title is-5">2.2 Labels from LLMs for GNNs</h4>

        <ul>
          <li><a href="https://arxiv.org/pdf/2403.01121"><b>OpenGraph: Towards Open Graph Foundation Models</b></a></li>
          <li><a href="https://arxiv.org/pdf/2310.04668">Label-free node classification on graphs with large language models (LLMs)</a></li>
          <li><a href="https://arxiv.org/pdf/2402.15183v1"><b>GraphEdit: Large Language Models for Graph Structure Learning</b></a></li>
          <li><a href="https://arxiv.org/pdf/2310.15950"><b>Representation learning with large language models for recommendation</b></a></li>
        </ul>

        <br />

        <h3 class="title is-4">Section 3: LLMs-Graphs Intergration</h3>
        <h4 class="title is-5">3.1 Alignment between GNNs and LLMs</h4>
        
        <ul>
          <li><a href="https://arxiv.org/pdf/2209.05481">A molecular multimodal foundation model associating molecule graphs with natural language</a></li>
          <li><a href="https://arxiv.org/pdf/2305.14321">ConGraT: Self-supervised contrastive pretraining for joint graph and text embeddings</a></li>
          <li><a href="https://arxiv.org/pdf/2307.10230">Prompt tuning on graph-augmented low-resource text classification</a></li>
          <li><a href="https://arxiv.org/pdf/2310.15109">GRENADE: Graph-Centric Language Model for Self-Supervised Representation Learning on Text-Attributed Graphs</a></li>
          <li><a href="https://arxiv.org/pdf/2212.10789">Multi-modal molecule structure–text model for text-based retrieval and editing</a></li>
          <li><a href="https://arxiv.org/pdf/2310.12580">Pretraining language models with text-attributed heterogeneous graphs</a></li>
          <li><a href="https://arxiv.org/pdf/2210.14709">Learning on large-scale text-attributed graphs via variational inference</a></li>
        </ul>

        <br />

        <h4 class="title is-5">3.2 Fusion Training of GNNs and LLMs</h4>

        <ul>
          <li><a href="https://arxiv.org/pdf/2201.08860">GreaseLM: Graph reasoning enhanced language models for question answering</a></li>
          <li><a href="https://arxiv.org/pdf/2310.18152v3">Disentangled representation learning with large language models for text-attributed graphs</a></li>
          <li><a href="https://arxiv.org/pdf/2401.15569">Efficient Tuning and Inference for Large Language Models on Textual Graphs</a></li>
          <li><a href="https://arxiv.org/pdf/2402.12984v1">Can GNN be Good Adapter for LLMs?</a></li>
        </ul>

        <br />

        <h4 class="title is-5">3.3 LLMs Agent for Graphs</h4>

        <ul>
          <li><a href="https://arxiv.org/pdf/2310.16421">Graph Agent: Explicit Reasoning Agent for Graphs</a></li>
          <li><a href="https://arxiv.org/pdf/2403.08593">Call Me When Necessary: LLMs can Efficiently and Faithfully Reason over Structured Environments</a></li>
          <li><a href="https://arxiv.org/pdf/2310.01061">Reasoning on graphs: Faithful and interpretable large language model reasoning</a></li>
        </ul>

        <br />

        <h3 class="title is-4">Section 4: LLMs-Only</h3>
        <h4 class="title is-5">4.1 Tuning-free</h4>
        
        <ul>
          <li><a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/622afc4edf2824a1b6aaf5afe153fa93-Paper-Conference.pdf">Can language models solve graph problems in natural language?</a></li>
          <li><a href="https://arxiv.org/pdf/2305.15066">GPT4Graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking</a></li>
          <li><a href="https://arxiv.org/pdf/2310.04944">BeyondText: A Deep Dive into Large Language Models’ Ability on Understanding Graph Data</a></li>
          <li><a href="https://arxiv.org/pdf/2307.03393">Exploring the potential of large language models (llms) in learning on graphs</a></li>
          <li><a href="https://arxiv.org/pdf/2310.01089">Graphtext: Graph reasoning in text space</a></li>
          <li><a href="https://arxiv.org/pdf/2310.04560">Talk like a graph: Encoding graphs for large language models</a></li>
          <li><a href="https://arxiv.org/pdf/2310.17110">LLM4DyG: Can Large Language Models Solve Problems on Dynamic Graphs?</a></li>
          <li><a href="https://arxiv.org/pdf/2311.09862">Which Modality should I use–Text, Motif, or Image?: Understanding Graphs with Large Language Models</a></li>
          <li><a href="https://arxiv.org/pdf/2312.10372">When Graph Data Meets Multimodal: A New Paradigm for Graph Understanding and Reasoning</a></li>
        </ul>

        <br />

        <h4 class="title is-5">4.2 Tuning-required</h4>

        <ul>
          <li><a href="https://arxiv.org/pdf/2308.07134">Natural language is all a graph needs</a></li>
          <li><a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/2ac879d1865475a7abc8dfc7a9c15c27-Paper-Conference.pdf">Walklm: A uniform language model fine-tuning framework for attributed graph embedding</a></li>
          <li><a href="https://arxiv.org/pdf/2402.08170v1">LLaGA: Large Language and Graph Assistant</a></li>
          <li><a href="https://arxiv.org/pdf/2402.08785">InstructGraph: Boosting Large Language Models via Graph-centric Instruction Tuning and Preference Alignment</a></li>
          <li><a href="https://arxiv.org/pdf/2402.11235">ZeroG: Investigating Cross-dataset Zero-shot Transferability in Graphs</a></li>
          <li><a href="https://arxiv.org/pdf/2402.16029">GraphWiz: An Instruction-Following Language Model for Graph Problems</a></li>
          <li><a href="https://arxiv.org/pdf/2403.04483">GraphInstruct: Empowering Large Language Models with Graph Understanding and Reasoning Capability</a></li>
          <li><a href="https://arxiv.org/pdf/2403.04780">MuseGraph: Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining</a></li>
        </ul>
        
        <br />

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Survey</h2>
        <ul>
          <ul>
            <li><a href="https://github.com/HKUDS/Awesome-LLM4Graph-Papers"><b>A Survey of Large Language Models for Graphs</b></a></li>
            <li><a href="https://arxiv.org/pdf/2312.02783">Large language models on graphs: A comprehensive survey</a></li>
            <li><a href="https://arxiv.org/pdf/2311.12399">A Survey of Graph Meets Large Language Model: Progress and Future Directions</a></li>
          </ul>
        </ul>
      </div>
    </div>

</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{ren2024survey,
  title={A Survey of Large Language Models for Graphs},
  author={Ren, Xubin and Tang, Jiabin and Yin, Dawei and Chawla, Nitesh and Huang, Chao},
  journal={arXiv preprint arXiv:2405.08011},
  year={2024}
}

@inproceedings{huang2024large,
  title={Large Language Models for Graphs: Progresses and Directions},
  author={Huang, Chao and Ren, Xubin and Tang, Jiabin and Yin, Dawei and Chawla, Nitesh},
  booktitle={Companion Proceedings of the ACM on Web Conference 2024},
  pages={1284--1287},
  year={2024}
}
  </code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://github.com/ACL2023-Retrieval-LM" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
